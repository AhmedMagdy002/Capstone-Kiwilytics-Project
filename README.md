# Capstone-Kiwilytics-Project– Daily Sales Revenue Pipeline

##  Project Objective
The goal of this project is to **automate an end-to-end data pipeline** that extracts daily sales data from a PostgreSQL database, calculates daily revenue, visualizes it as a time series plot, and schedules the entire workflow using **Apache Airflow**.

This project simulates a real-world data engineering scenario where pipelines must run automatically on a schedule, ensuring data is clean, transformed, and visualized for business use.

---

##  Pipeline Overview


- **1. Extract :**  
  This task connects to a PostgreSQL database and extracts the daily sales records.  
  The extracted data is saved locally as a **CSV file**, which acts as the raw data source for the rest of the pipeline.  

- **2. Data Quality Check :**  
  Before performing any transformations, the pipeline validates the extracted dataset to ensure that it’s not empty and contains the expected fields. This step helps catch potential data issues early.  

- **3. Transform :**  
  This task reads the extracted data, performs a **group-by aggregation** to calculate total revenue per day, and saves the results into a **cleaned CSV file**. This file represents the transformed dataset ready for analysis.  

- **4. Visualize:**  
  Finally, the pipeline generates a **time series plot** that shows daily total revenue trends. This visualization helps quickly identify sales patterns and track performance over time.


## DAG Graph

Below is the structure of the DAG inside the Airflow UI:

<img width="931" height="134" alt="Screenshot 2025-10-04 154104" src="https://github.com/user-attachments/assets/ff2e7538-4cdd-4511-8cb3-cf77700649ec" />

##  Sample Output Plot

sample daily revenue plot generated by the pipeline:
Here’s a<img width="1200" height="600" alt="daily_revenue" src="https://github.com/user-attachments/assets/775bcc86-1d58-4df8-a635-5175b61cf422" />



## Key Takeaways

- Demonstrates how to integrate Airflow with PostgreSQL

- Automates an ETL pipeline with proper task dependencies

- Shows the use of data quality checks in production workflows

- Generates automated visualizations for daily business insights
